{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "572efab5",
      "metadata": {},
      "source": [
        "# Part 1: CWSI ETL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8054b20c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# # Mark2 daily and hourly data ETL for CWSI Calculation and Save to S3\n",
        "# file load joined_df, lw_temp_3 and swdw into a dataframe used for CWSI calculation \n",
        "# and shared on quicksight and pwoerbi dashboard\n",
        "\n",
        "# !pip install scikit-learn\n",
        "# !pip install --force-reinstall boto3\n",
        "# !pip install psycopg2-binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "43845bc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import boto3\n",
        "import psycopg2\n",
        "from datetime import timedelta, datetime\n",
        "from sqlalchemy import create_engine, text\n",
        "from src.utils import df_from_s3, df_to_s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9e67e942",
      "metadata": {},
      "outputs": [],
      "source": [
        "# preload packages so only one ipynb used in scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "06a44d52",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# def df_from_s3(key, bucket, format=\"csv\", **kwargs):\n",
        "#     \"\"\"read csv from S3 as pandas df\n",
        "#     Arguments:\n",
        "#         key - key of file on S3\n",
        "#         bucket - bucket of file on S3\n",
        "#         **kwargs - additional keyword arguments to pass pd.read_ methods\n",
        "#     Returns:\n",
        "#         df - pandas df\n",
        "#     \"\"\"\n",
        "#     s3 = boto3.client(\"s3\")\n",
        "#     obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "#     body = obj[\"Body\"]\n",
        "#     if format == \"csv\":\n",
        "#         csv_string = body.read().decode(\"utf-8\")\n",
        "#         df = pd.read_csv(io.StringIO(csv_string), **kwargs)\n",
        "#     elif format == \"parquet\":\n",
        "#         bytes_obj = body.read()\n",
        "#         df = pd.read_parquet(io.BytesIO(bytes_obj), **kwargs)\n",
        "#     else:\n",
        "#         raise Exception(f\"format '{format}' not recognized\")\n",
        "#     return df\n",
        "\n",
        "\n",
        "# def df_to_s3(df, key, bucket, verbose=True, format=\"csv\"):\n",
        "#     if format == \"csv\":\n",
        "#         buffer = io.StringIO()\n",
        "#         df.to_csv(buffer, index=False)\n",
        "#     elif format == \"parquet\":\n",
        "#         buffer = io.BytesIO()\n",
        "#         df.to_parquet(buffer, index=False)\n",
        "#     else:\n",
        "#         raise Exception(f\"format '{format}' not recognized\")\n",
        "#     # write stream to S3\n",
        "#     s3 = boto3.client(\"s3\")\n",
        "#     s3.put_object(Bucket=bucket, Key=key, Body=buffer.getvalue())\n",
        "#     if verbose:\n",
        "#         print(f\"Uploaded file to s3://{bucket}/{key}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b57cd95e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ## Connect to Database\n",
        "def get_user_db_creds(user: str, environment: str):\n",
        "    client = boto3.client(\"secretsmanager\")\n",
        "    response = client.get_secret_value(SecretId=f\"{user}_db_creds_1\")\n",
        "    secret_db_creds = json.loads(response[\"SecretString\"])\n",
        "    db_info = {\n",
        "        \"user\": secret_db_creds[f\"user_{environment}\"],\n",
        "        \"password\": secret_db_creds[f\"password_{environment}\"],\n",
        "        \"host\": secret_db_creds[f\"host_{environment}\"],\n",
        "        \"db\": secret_db_creds[f\"db_{environment}\"],\n",
        "        \"port\": secret_db_creds[f\"port_{environment}\"],\n",
        "    }\n",
        "    return db_info\n",
        "\n",
        " \n",
        "def connect_db(dsn: str) -> str:\n",
        "    cnx = create_engine(dsn)\n",
        "    return cnx\n",
        " \n",
        "# * CWSI ETL Pipeline Integrated reference data and Mark3 data\n",
        "def read_ref_hourly(cnx, begin, end):\n",
        "    \n",
        "    schema_raw = 'hourly'\n",
        "    query_template_raw = \"\"\"    \n",
        "--may want to change me here\n",
        "\n",
        "-- 1. create meta data table\n",
        "WITH devmap AS (\n",
        "  SELECT *\n",
        "  FROM (VALUES\n",
        "('D003701', 'TWE_GB', 'L1'),\n",
        "('D003705', 'TWE_GB', 'L2'),\n",
        "('D003932', 'TWE_GB', 'H1'),\n",
        "('D003978', 'TWE_GB', 'H2'),\n",
        "('D003898', 'TWE_BV2', 'L1'),\n",
        "('D003960', 'TWE_BV2', 'L2'),\n",
        "('D003942', 'TWE_BV2', 'H1'),\n",
        "('D003943', 'TWE_BV2', 'H2')\n",
        ") AS t(device, site_id, source)\n",
        "), \n",
        "\n",
        "cte as (\n",
        "select\n",
        " \t   DATE_TRUNC('hour', ref_time) as ref_time, \n",
        "       site_id,\n",
        "       source,\n",
        "       avg(ref_tbelow) as ref_tbelow,\n",
        "       avg(ref_tsensor) as body_temp\n",
        "\n",
        "FROM   device_data.calval_ref_data\n",
        "where site_id in ('TWE_GB', 'TWE_BV2')\n",
        "and DATE_TRUNC('hour', ref_time)>'{start}' and DATE_TRUNC('hour', ref_time)  < '{end}'\n",
        "group by DATE_TRUNC('hour', ref_time), site_id, source\n",
        "ORDER  BY site_id, source, DATE_TRUNC('hour', ref_time)\n",
        "),\n",
        "\n",
        "-- 3. join meta data table with reference table\n",
        "cte1 as (\n",
        "SELECT c.*, d.device FROM devmap d\n",
        "join cte c\n",
        "using (site_id, source)\n",
        ")\n",
        ",\n",
        "--4. join mark and reference/meta table\n",
        "joined as (\n",
        "SELECT time, tair, tbelow, vpd, ea, precip, lat, long, c.* \n",
        "from device_data_alp.hourly d\n",
        "join cte1 c\n",
        "on c.device=d.device and c.ref_time =d.time\n",
        "--\n",
        "where \n",
        "d.device  in (\n",
        "'D003701', \n",
        "'D003705', \n",
        "'D003932', \n",
        "'D003978', \n",
        "'D003898', \n",
        "'D003960', \n",
        "'D003942', \n",
        "'D003943' )\n",
        "order by time\n",
        ")\n",
        "-- 5 join soil data\n",
        "SELECT  j.*,  moisture_0_mean,  moisture_2_mean,moisture_4_mean,moisture_6_mean, moisture_8_mean, salinity_0_mean,  temp_0_mean\n",
        "FROM device_data_alp.sentek_hourly s\n",
        "join joined j\n",
        "on j.device=s.device and j.time=s.time\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    sql_query = query_template_raw.format(schema=schema_raw, \\\n",
        "                                         start=begin, end=end)\n",
        "\n",
        "    df = pd.read_sql_query(sql_query, cnx)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "61c6fb93",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_ref_additional(cnx, begin, end, devices):\n",
        "    schema_raw = 'hourly'\n",
        "    query_template_raw = \"\"\"\n",
        "--may want to change me here\n",
        "\n",
        "SELECT date_trunc('hour', time) AS time, device, avg(swdw) AS swdw\n",
        "FROM device_data_alp.calibrated \n",
        "WHERE device IN ({devices})\n",
        "AND time > '{start}' AND time < '{end}'\n",
        "GROUP BY date_trunc('hour', time), device\n",
        "ORDER BY device, time;\n",
        "\"\"\".format(devices=devices, start=begin, end=end)\n",
        "\n",
        "    sql_query = query_template_raw.format(schema=schema_raw, \\\n",
        "                                         start=begin, end=end)\n",
        "\n",
        "    df = pd.read_sql_query(sql_query, cnx)\n",
        "\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "012b6d2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_temp3(cnx, begin, end, devices):\n",
        "    schema_raw = 'hourly'\n",
        "    query_template_raw = \"\"\"\n",
        "--may want to change me here\n",
        "\n",
        "SELECT date_trunc('hour', r.time) AS time, r.device, avg(r.lw_temp_3) AS lw_temp_3\n",
        "FROM device_data_alp.raw r \n",
        "WHERE device IN ({devices}) \n",
        "AND time > '{start}' AND time < '{end}'\n",
        "GROUP BY date_trunc('hour', r.time), r.device\n",
        "ORDER BY r.device, time;\n",
        "\"\"\".format(devices=devices, start=begin, end=end)\n",
        "\n",
        "    sql_query = query_template_raw.format(schema=schema_raw, \\\n",
        "                                         start=begin, end=end)\n",
        "\n",
        "    df = pd.read_sql_query(sql_query, cnx)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2574b734",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_irrigation(cnx, begin, end, devices):\n",
        "    schema_raw = 'hourly'\n",
        "    query_template_raw = \"\"\"\n",
        "--may want to change me here\n",
        "\n",
        "SELECT time,device, duration_seconds\n",
        "FROM device_data_alp.irrigation_runtime_hourly r \n",
        "WHERE device IN ({devices}) \n",
        "AND time > '{start}' AND time < '{end}'\n",
        "ORDER BY r.device, time;\n",
        "\"\"\".format(devices=devices, start=begin, end=end)\n",
        "\n",
        "    sql_query = query_template_raw.format(schema=schema_raw, \\\n",
        "                                         start=begin, end=end)\n",
        "\n",
        "    df = pd.read_sql_query(sql_query, cnx)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "48f31c59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# retrieve personal tocken from arable secrete Manager\n",
        "# --may want to change me here\n",
        "dsn=get_user_db_creds('hong_tang', 'adse')\n",
        "sqlalchemy_dsn = 'postgresql://{user}:{password}@{host}:{port}/{db}'.format(**dsn)\n",
        "\n",
        " \n",
        "pg_conn = connect_db(sqlalchemy_dsn)\n",
        "pg_conn\n",
        "\n",
        " \n",
        "# Define start and end dates\n",
        "start_date = '2023-03-25'\n",
        "# end_date = datetime.today().strftime('%Y-%m-%d')  # Get today's date\n",
        "end_date = '2023-06-20'\n",
        "bucket_name = 'arable-adse-dev'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6e7c9158",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'OptionEngine' object has no attribute 'execute'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m joined_df \u001b[39m=\u001b[39m read_ref_hourly(pg_conn, start_date, end_date)\n\u001b[1;32m      2\u001b[0m joined_df[\u001b[39m'\u001b[39m\u001b[39mref_time\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(joined_df[\u001b[39m'\u001b[39m\u001b[39mref_time\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m joined_df[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(joined_df[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m])\n",
            "Cell \u001b[0;32mIn[12], line 94\u001b[0m, in \u001b[0;36mread_ref_hourly\u001b[0;34m(cnx, begin, end)\u001b[0m\n\u001b[1;32m     24\u001b[0m     query_template_raw \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39m    \u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[39m--may want to change me here\u001b[39m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     91\u001b[0m     sql_query \u001b[39m=\u001b[39m query_template_raw\u001b[39m.\u001b[39mformat(schema\u001b[39m=\u001b[39mschema_raw, \\\n\u001b[1;32m     92\u001b[0m                                          start\u001b[39m=\u001b[39mbegin, end\u001b[39m=\u001b[39mend)\n\u001b[0;32m---> 94\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql_query(sql_query, cnx)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
            "File \u001b[0;32m~/Documents/langchain_prototypes/langenv/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    398\u001b[0m     sql,\n\u001b[1;32m    399\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    400\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    401\u001b[0m     coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    402\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    403\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    404\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    405\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/langchain_prototypes/langenv/lib/python3.9/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1561\u001b[0m columns \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/langchain_prototypes/langenv/lib/python3.9/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnectable\u001b[39m.\u001b[39;49mexecution_options()\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OptionEngine' object has no attribute 'execute'"
          ]
        }
      ],
      "source": [
        "joined_df = read_ref_hourly(pg_conn, start_date, end_date)\n",
        "joined_df['ref_time'] = pd.to_datetime(joined_df['ref_time'])\n",
        "\n",
        "joined_df['time'] = pd.to_datetime(joined_df['time'])\n",
        "joined_df.sort_values(['device', 'time'])\n",
        "\n",
        "path = f'Carbon Project/Stress Index/UCD_Almond/Joined_df_hourly.csv' #ET{device}_mark_df_daily.csv\n",
        "df_to_s3( joined_df, path, bucket_name, format ='csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "6e9d5369",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'OptionEngine' object has no attribute 'execute'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m devices_list \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003701\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003705\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003932\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003978\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003898\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003960\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003942\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mD003943\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m swdw_df \u001b[39m=\u001b[39m read_ref_additional(pg_conn, start_date, end_date, devices_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m swdw_df[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(swdw_df[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m swdw_df \u001b[39m=\u001b[39m swdw_df[swdw_df[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnotnull()]\n",
            "\u001b[1;32m/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb Cell 13\u001b[0m in \u001b[0;36mread_ref_additional\u001b[0;34m(cnx, begin, end, devices)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     query_template_raw \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m--may want to change me here\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mORDER BY device, time;\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mformat(devices\u001b[39m=\u001b[39mdevices, start\u001b[39m=\u001b[39mbegin, end\u001b[39m=\u001b[39mend)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     sql_query \u001b[39m=\u001b[39m query_template_raw\u001b[39m.\u001b[39mformat(schema\u001b[39m=\u001b[39mschema_raw, \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                                          start\u001b[39m=\u001b[39mbegin, end\u001b[39m=\u001b[39mend)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql_query(sql_query, cnx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hongtang/Documents/CWSI_Tbelow_From_Rajen/TWC_CWSI_ETL_Modeling.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
            "File \u001b[0;32m~/Notebook/deep-learning-flower-identifier/pydl/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    398\u001b[0m     sql,\n\u001b[1;32m    399\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    400\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    401\u001b[0m     coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    402\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    403\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    404\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    405\u001b[0m )\n",
            "File \u001b[0;32m~/Notebook/deep-learning-flower-identifier/pydl/lib/python3.9/site-packages/pandas/io/sql.py:1560\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \n\u001b[1;32m   1557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 1560\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1561\u001b[0m columns \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m   1563\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Notebook/deep-learning-flower-identifier/pydl/lib/python3.9/site-packages/pandas/io/sql.py:1405\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1404\u001b[0m     \u001b[39m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnectable\u001b[39m.\u001b[39;49mexecution_options()\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OptionEngine' object has no attribute 'execute'"
          ]
        }
      ],
      "source": [
        "devices_list = \"'D003701', 'D003705', 'D003932', 'D003978', 'D003898', 'D003960', 'D003942', 'D003943'\"\n",
        "swdw_df = read_ref_additional(pg_conn, start_date, end_date, devices_list)\n",
        "swdw_df['time'] = pd.to_datetime(swdw_df['time'])\n",
        "\n",
        "swdw_df = swdw_df[swdw_df['time'].notnull()]\n",
        "swdw_df['device'] = swdw_df['device'].astype('category')\n",
        "swdw_cleaned_df = swdw_df.groupby(['time', 'device']).agg({'swdw': 'mean'}).reset_index()\n",
        "swdw_cleaned_df = swdw_cleaned_df.dropna(subset=['swdw'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8ac63f0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file to s3://arable-adse-dev/Carbon Project/Stress Index/UCD_Almond/swdw_df_hourly.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>device</th>\n",
              "      <th>lw_temp_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-03-25 00:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>12.904567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-03-25 01:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>9.762093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-03-25 02:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>7.881252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-03-25 03:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>7.652340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-03-25 04:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>7.180525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2023-03-25 05:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>6.800452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2023-03-25 06:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>6.838397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2023-03-25 07:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>6.607955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2023-03-25 08:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>6.390898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2023-03-25 09:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>6.321740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2023-03-25 10:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>5.057135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2023-03-25 11:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>4.162240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2023-03-25 12:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>3.425018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2023-03-25 13:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>3.129120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2023-03-25 14:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>3.371344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2023-03-25 15:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>6.674395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2023-03-25 16:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>10.286948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2023-03-25 17:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>13.844492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2023-03-25 18:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>16.702389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2023-03-25 19:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>19.659210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2023-03-25 20:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>19.977047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2023-03-25 21:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>20.858739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2023-03-25 22:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>19.725988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2023-03-25 23:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>17.278014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2023-03-26 00:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>14.390481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>2023-03-26 01:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>10.521893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>2023-03-26 02:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>8.133342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>2023-03-26 03:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>7.337889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2023-03-26 04:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>7.077973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2023-03-26 05:00:00+00:00</td>\n",
              "      <td>D003701</td>\n",
              "      <td>5.575475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        time   device  lw_temp_3\n",
              "0  2023-03-25 00:00:00+00:00  D003701  12.904567\n",
              "1  2023-03-25 01:00:00+00:00  D003701   9.762093\n",
              "2  2023-03-25 02:00:00+00:00  D003701   7.881252\n",
              "3  2023-03-25 03:00:00+00:00  D003701   7.652340\n",
              "4  2023-03-25 04:00:00+00:00  D003701   7.180525\n",
              "5  2023-03-25 05:00:00+00:00  D003701   6.800452\n",
              "6  2023-03-25 06:00:00+00:00  D003701   6.838397\n",
              "7  2023-03-25 07:00:00+00:00  D003701   6.607955\n",
              "8  2023-03-25 08:00:00+00:00  D003701   6.390898\n",
              "9  2023-03-25 09:00:00+00:00  D003701   6.321740\n",
              "10 2023-03-25 10:00:00+00:00  D003701   5.057135\n",
              "11 2023-03-25 11:00:00+00:00  D003701   4.162240\n",
              "12 2023-03-25 12:00:00+00:00  D003701   3.425018\n",
              "13 2023-03-25 13:00:00+00:00  D003701   3.129120\n",
              "14 2023-03-25 14:00:00+00:00  D003701   3.371344\n",
              "15 2023-03-25 15:00:00+00:00  D003701   6.674395\n",
              "16 2023-03-25 16:00:00+00:00  D003701  10.286948\n",
              "17 2023-03-25 17:00:00+00:00  D003701  13.844492\n",
              "18 2023-03-25 18:00:00+00:00  D003701  16.702389\n",
              "19 2023-03-25 19:00:00+00:00  D003701  19.659210\n",
              "20 2023-03-25 20:00:00+00:00  D003701  19.977047\n",
              "21 2023-03-25 21:00:00+00:00  D003701  20.858739\n",
              "22 2023-03-25 22:00:00+00:00  D003701  19.725988\n",
              "23 2023-03-25 23:00:00+00:00  D003701  17.278014\n",
              "24 2023-03-26 00:00:00+00:00  D003701  14.390481\n",
              "25 2023-03-26 01:00:00+00:00  D003701  10.521893\n",
              "26 2023-03-26 02:00:00+00:00  D003701   8.133342\n",
              "27 2023-03-26 03:00:00+00:00  D003701   7.337889\n",
              "28 2023-03-26 04:00:00+00:00  D003701   7.077973\n",
              "29 2023-03-26 05:00:00+00:00  D003701   5.575475"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = f'Carbon Project/Stress Index/UCD_Almond/swdw_df_hourly.csv' #ET{device}_mark_df_daily.csv\n",
        "df_to_s3( swdw_cleaned_df, path, bucket_name, format ='csv')\n",
        "\n",
        "temp3_df = read_temp3(pg_conn, start_date, end_date, devices_list)\n",
        "temp3_df['time'] = pd.to_datetime(temp3_df['time'])\n",
        "\n",
        "temp3_df['time']=temp3_df['time'].replace('NaT', '-999')\n",
        "temp3_df['device'] = temp3_df['device'].astype('category')\n",
        "temp3_df.sort_values(['device', 'time']).head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8015d6c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file to s3://arable-adse-dev/Carbon Project/Stress Index/UCD_Almond/lw_temp3_df_hourly.csv\n"
          ]
        }
      ],
      "source": [
        "path = f'Carbon Project/Stress Index/UCD_Almond/lw_temp3_df_hourly.csv' #ET{device}_mark_df_daily.csv\n",
        "df_to_s3( temp3_df, path, bucket_name, format ='csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6038ed25",
      "metadata": {},
      "outputs": [],
      "source": [
        "irg_df = read_irrigation(pg_conn, start_date, end_date, devices_list)\n",
        "irg_df['time'] = pd.to_datetime(irg_df['time'])\n",
        "irg_df = irg_df[irg_df['time'].notnull()]\n",
        "irg_df['device'] = irg_df['device'].astype('category')\n",
        "path='Carbon Project/Stress Index/UCD_Almond/lw_temp3_df_hourly.csv'\n",
        "temp3_df=df_from_s3(path,bucket_name)\n",
        "path='Carbon Project/Stress Index/UCD_Almond/swdw_df_hourly.csv'\n",
        "swdw_df=df_from_s3(path,bucket_name)\n",
        "path='Carbon Project/Stress Index/UCD_Almond/Joined_df_hourly.csv'\n",
        "joined_df=df_from_s3(path,bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "459e4134",
      "metadata": {},
      "outputs": [],
      "source": [
        "temp3_df.shape, swdw_df.shape, joined_df.shape,irg_df.shape\n",
        "[temp3_df, swdw_df, joined_df, irg_df] = [df.apply(lambda x: pd.to_datetime(x) if x.name == 'time' else x) for df in [temp3_df, swdw_df, joined_df, irg_df]]\n",
        "[temp3_df, swdw_df, joined_df, irg_df] = [df.apply(lambda x: x.astype('category') if x.name == 'device' else x) for df in [temp3_df, swdw_df, joined_df, irg_df]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5a1a48fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df = joined_df.merge(swdw_df, on=['time', 'device']).merge(temp3_df, on=['time', 'device'])\n",
        "merged_df = merged_df.merge(irg_df, on=['time', 'device'], how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "00aa4729",
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df['duration_seconds'] = merged_df['duration_seconds'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2e61d3bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file to s3://arable-adse-dev/Carbon Project/Stress Index/UCD_Almond/Joined_ref_df_hourly.csv\n"
          ]
        }
      ],
      "source": [
        "bucket_name = 'arable-adse-dev'\n",
        "path = f'Carbon Project/Stress Index/UCD_Almond/Joined_ref_df_hourly.csv' #ET{device}_mark_df_daily.csv\n",
        "df_to_s3( merged_df, path, bucket_name, format ='csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0da6df",
      "metadata": {},
      "source": [
        "# Part 2: CWSI Feature Engineering and Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e4ffbdd9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.1.3\n"
          ]
        }
      ],
      "source": [
        "import nbformat\n",
        "print(nbformat.__version__)\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "# from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# import seaborn as sns\n",
        "import boto3\n",
        "from botocore.session import Session\n",
        "import pandas.io.sql as psql\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "839a8581",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the path to calval-etl to sys.path\n",
        "path_to_calval_etl = '/home/ec2-user/SageMaker/calval-etl'\n",
        "sys.path.append(path_to_calval_etl)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4755d3f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import specific functions from src.math module\n",
        "# from src.math import NDVI_, EVI_, crop_kH, SlpInt, lr_vpd_tdif, esat_, upper_limit, lower_limit\n",
        "# from src.utils import df_from_s3, df_to_s3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b07dc95",
      "metadata": {},
      "outputs": [],
      "source": [
        "#preload functions\n",
        "\n",
        "## NDVI and EVI functions \n",
        "def NDVI_(df):\n",
        "    \"\"\"NDVI\n",
        "    \"\"\"\n",
        "    df['ndvi'] = (df['b6uw'] - df['b4uw']) / (df['b6uw'] + df['b4uw'])\n",
        "    return df\n",
        "\n",
        "def EVI_(df):\n",
        "    \"\"\"EVI\n",
        "    \"\"\"\n",
        "    df['evi'] = 2.5 *(df['b6uw'] - df['b4uw']) / (df['b6uw'] + (6 * df['b4uw'] - 7.5 * df['b1uw']) +1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def crop_kH(df):\n",
        "    \"\"\"\n",
        "    Calculates the crop coefficient and hourly actual evapotranspiration for a crop using NDVI and actual ET values.\n",
        "\n",
        "    Args:\n",
        "        df: A Pandas DataFrame with columns 'ndvi' and 'et'.\n",
        "\n",
        "    Returns:\n",
        "        A new DataFrame with columns 'kcb_hr' and 'new_etc_hr'. 'kcb_hr' represents the crop coefficient and 'new_etc_hr' represents \n",
        "        the hourly actual evapotranspiration calculated as the product of 'et' and 'kcb_hr'.\n",
        "    \"\"\"\n",
        "    df['kcb_hr'] =  0.176 + 1.325 * df['ndvi'] - 1.466 * df['ndvi'] ** 2 + 1.146 * df['ndvi'] ** 3\n",
        "    df['new_etc_hr'] = df['et'] * df [\"kcb_hr\"] \n",
        "    return \n",
        "\n",
        "def SlpInt(df):\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    mask = (df['date'] > '2021-06-01') & (df['date'] <= '2021-07-30')\n",
        "    df = ET75_daily.loc[mask_m]\n",
        "    df_sub = {'vpd': df['vpd'], 'diff':df['ref_tbelow']- df['tair']}\n",
        "    df_sub =pd.DataFrame.from_dict(df_sub)\n",
        "    df_sub = df_sub[df_sub['diff'].notna()]\n",
        "    x= df_sub.iloc[:,0].values.reshape(-1,1)\n",
        "    y= df_sub.iloc[:,1].values.reshape(-1,1)\n",
        "    lr= LinearRegression()\n",
        "    m1= lr.fit(x, y)\n",
        "    Y_pred = lr.predict(x)\n",
        "    plt.scatter(x, y)\n",
        "    plt.plot(x, Y_pred, color='red')\n",
        "    plt.title('Apogee data_R ')\n",
        "    plt.ylabel ('tbelow - tair')\n",
        "    plt.xlabel ('vpd')\n",
        "    plt.show()\n",
        "    b=m1.intercept_\n",
        "    a=m1.coef_\n",
        "    print(a,b)\n",
        "    return\n",
        "\n",
        "def lr_vpd_tdif(df):\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    ''' \n",
        "    Fits a linear regression model to two columns of a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: A DataFrame with two columns representing independent variable x and dependent variable y.\n",
        "\n",
        "    Returns:\n",
        "        A tuple with four elements: x, Y_pred, coef, and intercept. x is the input independent variable, Y_pred is\n",
        "        the predicted dependent variable, coef is the slope of the regression line, and intercept is the y-intercept.\n",
        "        '''\n",
        "    x = df.iloc[:,0].values.reshape(-1,1)\n",
        "    y= df.iloc[:,1].values.reshape(-1,1)\n",
        "\n",
        "    lr = LinearRegression()\n",
        "    ## ideally from Apogee data to establish regression\n",
        "\n",
        "    m1= lr.fit(x, y)\n",
        "    Y_pred = lr.predict(x)\n",
        "    return x, Y_pred, m1.coef_, m1.intercept_\n",
        "\n",
        "def esat_(xT):  # pragma: no cover\n",
        "    \"\"\" saturation vapor pressure: kPa\n",
        "        return 0.611*exp(17.27*xT/(xT+237)) # Monteith formulation\n",
        "    :param xT:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return (617.4 + 42.22 * xT + 1.675 * xT ** 2 + 0.01408 * xT ** 3 +\n",
        "            0.0005818 * xT ** 4)/1000\n",
        "\n",
        "def upper_limit(df):\n",
        "    cp = 1013 # heat capacity of air\n",
        "    d = 1.225 #density of air\n",
        "    zm = 4.5 # height of wind measurement \n",
        "    chmax = 4\n",
        "    dp = 2*(chmax/3)\n",
        "    zom = 0.123 * chmax\n",
        "    zoh = 0.1 * zom \n",
        "    ra=(math.log((zm-dp)/zom)*math.log((zm-dp)/zoh))/(0.41*0.41*2)\n",
        "    #ra=14.16\n",
        "    print (ra)\n",
        "    ul_m = (ra * (df[\"swdw\"]))/( cp * d )\n",
        "    return ul_m\n",
        "\n",
        "def lower_limit(df): \n",
        "    chmax = 4\n",
        "    dp = 2*(chmax/3)\n",
        "    zom = 0.123 * chmax\n",
        "    zoh = 0.1 * zom\n",
        "    print(zoh)\n",
        "    gamma =  101.325 *(101.3*(((293-0.0065 *36.6)/293)** 5.26))/ (0.622 * (2503 -2.39 * df.tair)* 100)\n",
        "    #print (gamma)\n",
        "    delta = (4098 * (0.6108 * np.exp((17.27 * df.tair) / (df.tair + 237.3)) / ((df.tair + 237.3) ** 2)))\n",
        "    #print (delta)\n",
        "    es =0.6108 *(np.exp((17.27*df.tair)/(df.tair+237.3)))\n",
        "    ea = df['ea']\n",
        "    ll_m = ((df.ul_m * gamma)/delta + zoh) - ((es - ea) / (delta + zoh)) \n",
        "    return ll_m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ea96cac1",
      "metadata": {},
      "outputs": [],
      "source": [
        "session = Session()\n",
        "# define critical temperature for DACT calculation\n",
        "Tcritical = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "eee75b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process ET DataFrame by converting columns to appropriate data types and taking the average for each 30-minute interval\n",
        "def process_ET(df):\n",
        "    df_p = df.copy()\n",
        "    df_p['local_time'] = df_p['local_time'].astype('datetime64[ns]').dt.round('30min')\n",
        "    cols = [i for i in df_p.columns if i not in ['local_time', 'device', 'treatment', 'location', 'update_time', 'create_time']]\n",
        "    for col in cols:\n",
        "        df_p[col] = pd.to_numeric(df_p[col], errors='coerce')\n",
        "    df_p1 = df_p.groupby('local_time').mean().reset_index()\n",
        "    return df_p1\n",
        "\n",
        "# %%\n",
        "def DACT_DANS(df, Tcritical):\n",
        "    \"\"\"\n",
        "    Calculates DACT and DANS based on the provided DataFrame and critical temperature.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pandas.DataFrame): DataFrame containing temperature data.\n",
        "    - Tcritical (float): Critical temperature value (in Celsius).\n",
        "\n",
        "    Returns:\n",
        "    - df (pandas.DataFrame): DataFrame with added columns for DACT and DANS.\n",
        "\n",
        "    If an error occurs during the calculation of DANS, an error message is displayed.\n",
        "\n",
        "    Note: This function modifies the input DataFrame by adding the 'DACT' and 'DANS' columns.\n",
        "    # define Tcritical=20C from https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=2505&context=usdaarsfacpub\n",
        "    \"\"\"\n",
        "    \n",
        "   \n",
        "    df['DACT'] = (df['tbelow'] - Tcritical).clip(lower=0)\n",
        "   \n",
        "    try:\n",
        "        if (df['tair'] != 0).all() or df['tair'].notnull().all():\n",
        "            df['DANS'] = (df['tair'] - df['tbelow']) / df['tair']\n",
        "        else:\n",
        "            print('Check if tair is not null or equals to zero')\n",
        "    except:\n",
        "        print('An error occurred while calculating DANS. Please check your data.')\n",
        "    return df\n",
        "\n",
        "# %% Stress Time above Tcritical\n",
        "def Stress_Intensity(df):\n",
        "    '''\n",
        "    aggregate DACT daily sum to df_daily dataframe\n",
        "    and aggregate DACT >0 (defined as >0.00001) to get stressed hours\n",
        "    '''\n",
        "    df_daily=pd.DataFrame()\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    df.set_index('time', inplace=True)\n",
        "    \n",
        "    df_daily['device'] = df['device'].groupby(pd.Grouper(freq='D')).first()\n",
        "    df_daily['site_id'] = df['site_id'].groupby(pd.Grouper(freq='D')).first()\n",
        "    df_daily['source'] = df['source'].groupby(pd.Grouper(freq='D')).first()\n",
        "    df_daily['DACT_daily'] = df['DACT'].groupby(pd.Grouper(freq='D')).sum()\n",
        "    # use to compute daily hours when DACT greater than zero \n",
        "    df['DACTgtZero']=np.where(df['DACT']>0.00001, 1, 0)\n",
        "    df_daily['stressedHours'] = df['DACTgtZero'].groupby(pd.Grouper(freq='D')).sum()\n",
        "\n",
        "    df_daily.reset_index(inplace=True)\n",
        " \n",
        "    return df_daily\n",
        "\n",
        "# %%\n",
        "# Perform preprocessing steps on DataFrame, including calculating various columns based on existing columns\n",
        "def preprocessing(df, a, b):\n",
        "    df['LL'] = a + b * df['vpd']  # Define lower limit\n",
        "    df['VPG'] = (df['es'] * (a + df['tair'])) - df['es'] * df['tair']  # Compute VPG\n",
        "    df['UL_mod'] = (b - a) * abs(df['VPG'])  # ULmode\n",
        "    df['UL'] = df['tbelow'] + 5  # Upper limit\n",
        "    df['diff'] = df['tbelow'] - df['tair']\n",
        "    df['CWSI'] = (abs(df['diff'] - df['LL']) / (df['UL_mod'] - df['LL']))\n",
        "\n",
        "    window_size = 3 #moving averate of 3 hours\n",
        "    #compute pseudo CWSI by computing slopes of diff/vpd\n",
        "    df['diff_rolling_avg'] = df['diff'].rolling(window=window_size, min_periods=1).mean()\n",
        "    df['vpd_rolling_avg'] = df['vpd'].rolling(window=window_size, min_periods=1).mean()\n",
        "    df['pCWSI'] = df.apply(lambda x: x.diff_rolling_avg/x.vpd_rolling_avg, axis=1)\n",
        "\n",
        "    #calculate DACT and DANS\n",
        "    DACT_DANS(df, Tcritical)\n",
        "\n",
        "    df_daily=pd.DataFrame()\n",
        "    # calculate stress intensity\n",
        "    df_daily = Stress_Intensity(df)\n",
        "\n",
        "    return df, df_daily\n",
        "\n",
        "def get_solar_time(longitude, utc_time):\n",
        "    # Convert utc_time to pandas Timestamp object\n",
        "    utc_time = pd.to_datetime(utc_time)\n",
        "    # Calculate the hour offset based on the longitude\n",
        "    hour_offset = round(longitude / 15)\n",
        "    hour_offset = pd.to_timedelta(hour_offset, unit = 'h')\n",
        "    # Apply the offset to the UTC time to get the solar time\n",
        "    solar_time = utc_time + hour_offset\n",
        "\n",
        "    return solar_time\n",
        "\n",
        "\n",
        "def create_features(field_df):\n",
        "# Calculate the 'tbelow-tair' column by subtracting 'tair' from 'tbelow'\n",
        "    field_df['ref_tbelow-tair'] = field_df['ref_tbelow'] - field_df['tair']\n",
        "    field_df['es'] = esat_(field_df['tair'])\n",
        "    # field_df['tzconvert_time1'] = field_df['time'].dt.tz_convert('America/Los_Angeles')\n",
        "    # field_df['fntrans_time'] = solar_noon_(field_df['time'], field_df['long'])\n",
        "    field_df['solar_time'] = get_solar_time( field_df['long'], field_df['time'])\n",
        "    field_df['solarnoon'] = (field_df['solar_time'].dt.hour >= 12) & (field_df['solar_time'].dt.hour < 16)\n",
        "    field_df['solarnoon'] = field_df['solarnoon'].astype(int)\n",
        "    return field_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "67406522",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "#dataframe is pulled using SQL query directly using: CWSI_Ref_Mark_Joined_data_ETL.py\n",
        "bucket_name = 'arable-adse-dev'\n",
        "path = 'Carbon Project/Stress Index/UCD_Almond/Joined_ref_df_hourly.csv'\n",
        "\n",
        "# Load DataFrame from S3 bucket\n",
        "joined_df = df_from_s3(path, bucket_name, format='csv')\n",
        "\n",
        "# %%\n",
        "# change columns names to use following processing program\n",
        "field_df=joined_df.drop(['ref_time', 'body_temp'], axis=1)\n",
        "\n",
        "# Create the 'tbelow-tair' and es\n",
        "create_features(field_df)\n",
        "# Drop rows with missing values\n",
        "field_df.dropna(inplace=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2c591eff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TWE_GB L1 2.3976882921363454 0.3049926375546108 (1416, 37) (59, 6)\n",
            "TWE_GB L2 -0.4728982971585913 2.266635941721744 (2832, 37) (118, 6)\n",
            "TWE_GB H1 0.18857050986929116 1.8364802551618582 (4201, 37) (176, 6)\n",
            "TWE_GB H2 0.6014985570267919 2.7214604238331077 (5570, 37) (234, 6)\n",
            "TWE_BV2 L1 -1.2283247125698615 2.384452010309309 (6943, 37) (292, 6)\n",
            "TWE_BV2 L2 -1.542587167718385 3.483488642538392 (8316, 37) (350, 6)\n",
            "TWE_BV2 H1 -0.8168788393688431 3.3099098304120904 (9687, 37) (408, 6)\n",
            "TWE_BV2 H2 -0.8872635718918395 1.998208480873998 (11058, 37) (466, 6)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[['TWE_GB', 'L1', 2.3976882921363454, 0.3049926375546108],\n",
              " ['TWE_GB', 'L2', -0.4728982971585913, 2.266635941721744],\n",
              " ['TWE_GB', 'H1', 0.18857050986929116, 1.8364802551618582],\n",
              " ['TWE_GB', 'H2', 0.6014985570267919, 2.7214604238331077],\n",
              " ['TWE_BV2', 'L1', -1.2283247125698615, 2.384452010309309],\n",
              " ['TWE_BV2', 'L2', -1.542587167718385, 3.483488642538392],\n",
              " ['TWE_BV2', 'H1', -0.8168788393688431, 3.3099098304120904],\n",
              " ['TWE_BV2', 'H2', -0.8872635718918395, 1.998208480873998]]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%\n",
        "regline_list=[] # store regression parameters\n",
        "# export hourly dataframe for CWSI related calculation\n",
        "res_df = pd.DataFrame()\n",
        "\n",
        "# create daily sum for DACT as a metric for stress hours and intensity for sites\n",
        "res_daily= pd.DataFrame()\n",
        "\n",
        "sites = field_df.site_id.unique().tolist()\n",
        "\n",
        "#create CWSI\n",
        "for site in sites:\n",
        "    #select time window to compute m1_coef_, m1_intercept_\n",
        "    for source in ['L1','L2', 'H1', 'H2']:\n",
        "        solarnoon_mask=(field_df.solarnoon==1) & (field_df.time> '2023-06-01') & (field_df.time< '2023-06-29')\n",
        "        # compute slope and interception for solarnoon on each device\n",
        "        x, Y_pred, m1_coef_, m1_intercept_ = lr_vpd_tdif(field_df[(field_df['site_id'] == site) \\\n",
        "                                                                & (field_df['source'] == source) \\\n",
        "                                                                & solarnoon_mask][['vpd', 'ref_tbelow-tair']])\n",
        "        plotmask=(field_df['site_id'] == site) & (field_df['source'] == source)\n",
        "        _, _df_daily = preprocessing(field_df[plotmask], m1_coef_[0][0], m1_intercept_[0])\n",
        "        res_df = pd.concat([res_df, _])\n",
        "        res_daily = pd.concat([res_daily, _df_daily])\n",
        "        print(site, source, m1_coef_[0][0], m1_intercept_[0], res_df.shape, res_daily.shape)\n",
        "        regline_list.append([site, source, m1_coef_[0][0], m1_intercept_[0]])\n",
        "\n",
        "res_df = res_df.drop(['diff', 'diff_rolling_avg', 'vpd_rolling_avg'], axis=1)\n",
        "regline_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c3899bc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#reset index so the time index will be treated as a column in quicksight\n",
        "res_df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "807c6341",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded file to s3://arable-adse-dev/Carbon Project/Stress Index/UCD_Almond/field_cwsi_trial_daily.csv\n",
            "Uploaded file to s3://arable-adse-dev/Carbon Project/Stress Index/UCD_Almond/field_cwsi_trial.csv\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "bucket_name = 'arable-adse-dev'\n",
        "path = 'Carbon Project/Stress Index/UCD_Almond/field_cwsi_trial_daily.csv'\n",
        "# Save res_df to S3 bucket\n",
        "df_to_s3(res_daily, path, bucket_name, format='csv')\n",
        "\n",
        "\n",
        "# %%\n",
        "bucket_name = 'arable-adse-dev'\n",
        "path = 'Carbon Project/Stress Index/UCD_Almond/field_cwsi_trial.csv'\n",
        "# Save res_df to S3 bucket\n",
        "df_to_s3(res_df, path, bucket_name, format='csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fad2d0e4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myvenv",
      "language": "python",
      "name": "myvenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
